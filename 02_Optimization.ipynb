{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SciPy Optimization\n",
    "\n",
    "unconstrained optimization, global optimization, constrained optimization, SVM-style classification, curve fitting, activation fitting, root finding, and systems of nonlinear equations. Each topic includes practical examples with realistic data and clear numerical outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import optimize\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "def r2_score(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    ss_res = np.sum((y_true - y_pred) ** 2)\n",
    "    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n",
    "    return 1 - ss_res / ss_tot\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(np.mean((np.asarray(y_true) - np.asarray(y_pred)) ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Unconstrained Minimization\n",
    "Unconstrained minimization finds parameter values that minimize an objective when no explicit bounds or constraints are applied. It is common in model fitting and pricing decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1: Energy demand linear model\n",
      "BFGS parameters: intercept=50.196, slope=2.228, MSE=48.699\n",
      "Nelder-Mead parameters: intercept=50.196, slope=2.228, MSE=48.699\n",
      "\n",
      "Example 2: Retail pricing\n",
      "Optimal price: $78.33\n",
      "Expected units sold: 460.0\n",
      "Expected profit: $17633.33\n"
     ]
    }
   ],
   "source": [
    "rng = np.random.default_rng(42)\n",
    "temperature = np.linspace(10, 35, 120)\n",
    "energy = 48.0 + 2.35 * temperature + rng.normal(0, 9, size=temperature.size)\n",
    "\n",
    "def mse(params, x, y):\n",
    "    b0, b1 = params\n",
    "    pred = b0 + b1 * x\n",
    "    return np.mean((pred - y) ** 2)\n",
    "\n",
    "def mse_grad(params, x, y):\n",
    "    b0, b1 = params\n",
    "    err = (b0 + b1 * x) - y\n",
    "    n = y.size\n",
    "    return np.array([2.0 * err.sum() / n, 2.0 * np.dot(err, x) / n])\n",
    "\n",
    "bfgs_fit = optimize.minimize(mse, x0=[0.0, 0.0], args=(temperature, energy), jac=mse_grad, method=\"BFGS\")\n",
    "nelder_fit = optimize.minimize(mse, x0=[0.0, 0.0], args=(temperature, energy), method=\"Nelder-Mead\")\n",
    "\n",
    "print(\"Example 1: Energy demand linear model\")\n",
    "print(f\"BFGS parameters: intercept={bfgs_fit.x[0]:.3f}, slope={bfgs_fit.x[1]:.3f}, MSE={bfgs_fit.fun:.3f}\")\n",
    "print(f\"Nelder-Mead parameters: intercept={nelder_fit.x[0]:.3f}, slope={nelder_fit.x[1]:.3f}, MSE={nelder_fit.fun:.3f}\")\n",
    "\n",
    "def negative_profit(price):\n",
    "    demand = 1400.0 - 12.0 * price\n",
    "    return -((price - 40.0) * demand)\n",
    "\n",
    "price_solution = optimize.minimize_scalar(negative_profit, bounds=(40.0, 100.0), method=\"bounded\")\n",
    "best_price = price_solution.x\n",
    "best_demand = 1400.0 - 12.0 * best_price\n",
    "best_profit = -price_solution.fun\n",
    "\n",
    "print()\n",
    "print(\"Example 2: Retail pricing\")\n",
    "print(f\"Optimal price: ${best_price:.2f}\")\n",
    "print(f\"Expected units sold: {best_demand:.1f}\")\n",
    "print(f\"Expected profit: ${best_profit:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Global Optimization\n",
    "Global optimization searches for the best solution across the entire landscape and is useful when many local minima exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1: Rastrigin function\n",
      "Local search value: 0.000000 at [-0. -0.]\n",
      "Differential evolution value: 0.000000 at [ 0. -0.]\n",
      "\n",
      "Example 2: Himmelblau function\n",
      "Local search objective values: [np.float64(0.0), np.float64(0.0), np.float64(0.0)]\n",
      "Dual annealing value: 0.000000 at [-3.7793 -3.2832]\n"
     ]
    }
   ],
   "source": [
    "def rastrigin(x):\n",
    "    x = np.asarray(x)\n",
    "    return 10 * x.size + np.sum(x ** 2 - 10 * np.cos(2 * np.pi * x))\n",
    "\n",
    "bounds_rastrigin = [(-5.12, 5.12), (-5.12, 5.12)]\n",
    "local_rastrigin = optimize.minimize(rastrigin, x0=[3.5, 3.5], method=\"L-BFGS-B\", bounds=bounds_rastrigin)\n",
    "global_rastrigin = optimize.differential_evolution(rastrigin, bounds_rastrigin, seed=42, maxiter=250, polish=True)\n",
    "\n",
    "print(\"Example 1: Rastrigin function\")\n",
    "print(f\"Local search value: {local_rastrigin.fun:.6f} at {local_rastrigin.x}\")\n",
    "print(f\"Differential evolution value: {global_rastrigin.fun:.6f} at {global_rastrigin.x}\")\n",
    "\n",
    "def himmelblau(x):\n",
    "    x1, x2 = np.asarray(x)\n",
    "    return (x1 ** 2 + x2 - 11) ** 2 + (x1 + x2 ** 2 - 7) ** 2\n",
    "\n",
    "starts = np.array([[5.5, 5.5], [-5.0, 5.0], [0.5, -4.5]])\n",
    "local_himmel = [\n",
    "    optimize.minimize(himmelblau, x0=s, method=\"L-BFGS-B\", bounds=[(-6, 6), (-6, 6)])\n",
    "    for s in starts\n",
    "]\n",
    "annealed_himmel = optimize.dual_annealing(himmelblau, bounds=[(-6, 6), (-6, 6)], seed=42, maxiter=300)\n",
    "\n",
    "print()\n",
    "print(\"Example 2: Himmelblau function\")\n",
    "print(\"Local search objective values:\", [round(r.fun, 6) for r in local_himmel])\n",
    "print(f\"Dual annealing value: {annealed_himmel.fun:.6f} at {annealed_himmel.x}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Constrained Optimization\n",
    "Constrained optimization handles business or engineering limits while still finding the best objective value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1: Portfolio optimization\n",
      "Success: True\n",
      "Weights: {np.str_('Stock A'): np.float64(0.2629), np.str_('Stock B'): np.float64(0.2126), np.str_('Stock C'): np.float64(0.1531), np.str_('Stock D'): np.float64(0.1763), np.str_('Bond'): np.float64(0.1952)}\n",
      "Expected return: 0.1000\n",
      "Portfolio risk (std): 0.1584\n",
      "\n",
      "Example 2: Production planning\n",
      "Standard units: 260.0\n",
      "Premium units: 70.0\n",
      "Monthly profit: $14950.00\n"
     ]
    }
   ],
   "source": [
    "expected_returns = np.array([0.08, 0.12, 0.10, 0.15, 0.06])\n",
    "cov_matrix = np.array([\n",
    "    [0.04, 0.01, 0.02, 0.01, 0.005],\n",
    "    [0.01, 0.09, 0.03, 0.02, 0.01],\n",
    "    [0.02, 0.03, 0.06, 0.015, 0.008],\n",
    "    [0.01, 0.02, 0.015, 0.16, 0.02],\n",
    "    [0.005, 0.01, 0.008, 0.02, 0.02]\n",
    "])\n",
    "\n",
    "def portfolio_variance(weights):\n",
    "    return weights @ cov_matrix @ weights\n",
    "\n",
    "def portfolio_grad(weights):\n",
    "    return 2.0 * cov_matrix @ weights\n",
    "\n",
    "portfolio_constraints = [\n",
    "    {\"type\": \"eq\", \"fun\": lambda w: np.sum(w) - 1.0},\n",
    "    {\"type\": \"ineq\", \"fun\": lambda w: w @ expected_returns - 0.10}\n",
    "]\n",
    "portfolio_bounds = [(0.0, 0.6) for _ in range(5)]\n",
    "portfolio_start = np.full(5, 0.2)\n",
    "portfolio_solution = optimize.minimize(\n",
    "    portfolio_variance,\n",
    "    x0=portfolio_start,\n",
    "    jac=portfolio_grad,\n",
    "    method=\"SLSQP\",\n",
    "    bounds=portfolio_bounds,\n",
    "    constraints=portfolio_constraints\n",
    ")\n",
    "\n",
    "assets = np.array([\"Stock A\", \"Stock B\", \"Stock C\", \"Stock D\", \"Bond\"])\n",
    "weights = portfolio_solution.x\n",
    "\n",
    "print(\"Example 1: Portfolio optimization\")\n",
    "print(f\"Success: {portfolio_solution.success}\")\n",
    "print(\"Weights:\", dict(zip(assets, np.round(weights, 4))))\n",
    "print(f\"Expected return: {weights @ expected_returns:.4f}\")\n",
    "print(f\"Portfolio risk (std): {np.sqrt(portfolio_variance(weights)):.4f}\")\n",
    "\n",
    "def negative_monthly_profit(x):\n",
    "    standard_units, premium_units = x\n",
    "    return -(40.0 * standard_units + 65.0 * premium_units)\n",
    "\n",
    "production_constraints = [\n",
    "    {\"type\": \"ineq\", \"fun\": lambda x: 800.0 - (2.0 * x[0] + 4.0 * x[1])},\n",
    "    {\"type\": \"ineq\", \"fun\": lambda x: 500.0 - (1.0 * x[0] + 1.5 * x[1])}\n",
    "]\n",
    "production_bounds = [(0.0, 260.0), (0.0, 180.0)]\n",
    "production_solution = optimize.minimize(\n",
    "    negative_monthly_profit,\n",
    "    x0=[120.0, 80.0],\n",
    "    method=\"SLSQP\",\n",
    "    bounds=production_bounds,\n",
    "    constraints=production_constraints\n",
    ")\n",
    "\n",
    "standard_units, premium_units = production_solution.x\n",
    "\n",
    "print()\n",
    "print(\"Example 2: Production planning\")\n",
    "print(f\"Standard units: {standard_units:.1f}\")\n",
    "print(f\"Premium units: {premium_units:.1f}\")\n",
    "print(f\"Monthly profit: ${-production_solution.fun:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. SVM-like Classification\n",
    "This approach minimizes a regularized hinge-loss objective, which gives a maximum-margin style linear classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1: Balanced customer churn classification\n",
      "Weights: [1.5644 1.0158], bias: 0.1983\n",
      "Training accuracy: 0.9833\n",
      "\n",
      "Example 2: Imbalanced fraud screening\n",
      "Plain model recall for fraud class: 0.8667\n",
      "Class-weighted model recall for fraud class: 0.9778\n"
     ]
    }
   ],
   "source": [
    "def hinge_objective(params, X, y, c_pos=1.0, c_neg=1.0):\n",
    "    w = params[:-1]\n",
    "    b = params[-1]\n",
    "    margins = y * (X @ w + b)\n",
    "    sample_weights = np.where(y > 0, c_pos, c_neg)\n",
    "    hinge = np.maximum(0.0, 1.0 - margins)\n",
    "    return 0.5 * np.dot(w, w) + np.sum(sample_weights * hinge)\n",
    "\n",
    "def linear_predict(params, X):\n",
    "    scores = X @ params[:-1] + params[-1]\n",
    "    return np.where(scores >= 0.0, 1.0, -1.0)\n",
    "\n",
    "rng = np.random.default_rng(7)\n",
    "X_neg = rng.normal(loc=[-1.5, -1.0], scale=[1.0, 0.9], size=(120, 2))\n",
    "X_pos = rng.normal(loc=[1.3, 1.5], scale=[0.9, 1.0], size=(120, 2))\n",
    "X_balanced = np.vstack([X_neg, X_pos])\n",
    "y_balanced = np.hstack([-np.ones(len(X_neg)), np.ones(len(X_pos))])\n",
    "\n",
    "svm_balanced = optimize.minimize(\n",
    "    hinge_objective,\n",
    "    x0=np.zeros(3),\n",
    "    args=(X_balanced, y_balanced, 1.0, 1.0),\n",
    "    method=\"L-BFGS-B\"\n",
    ")\n",
    "pred_balanced = linear_predict(svm_balanced.x, X_balanced)\n",
    "acc_balanced = np.mean(pred_balanced == y_balanced)\n",
    "\n",
    "print(\"Example 1: Balanced customer churn classification\")\n",
    "print(f\"Weights: {np.round(svm_balanced.x[:-1], 4)}, bias: {svm_balanced.x[-1]:.4f}\")\n",
    "print(f\"Training accuracy: {acc_balanced:.4f}\")\n",
    "\n",
    "X_normal = rng.normal(loc=[0.0, 0.0], scale=[1.1, 1.0], size=(450, 2))\n",
    "X_fraud = rng.normal(loc=[2.6, 2.2], scale=[0.8, 0.7], size=(45, 2))\n",
    "X_imb = np.vstack([X_normal, X_fraud])\n",
    "y_imb = np.hstack([-np.ones(len(X_normal)), np.ones(len(X_fraud))])\n",
    "\n",
    "svm_plain = optimize.minimize(hinge_objective, x0=np.zeros(3), args=(X_imb, y_imb, 1.0, 1.0), method=\"L-BFGS-B\")\n",
    "svm_weighted = optimize.minimize(hinge_objective, x0=np.zeros(3), args=(X_imb, y_imb, 8.0, 1.0), method=\"L-BFGS-B\")\n",
    "\n",
    "pred_plain = linear_predict(svm_plain.x, X_imb)\n",
    "pred_weighted = linear_predict(svm_weighted.x, X_imb)\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == -1))\n",
    "    return tp / (tp + fn)\n",
    "\n",
    "print()\n",
    "print(\"Example 2: Imbalanced fraud screening\")\n",
    "print(f\"Plain model recall for fraud class: {recall(y_imb, pred_plain):.4f}\")\n",
    "print(f\"Class-weighted model recall for fraud class: {recall(y_imb, pred_weighted):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Curve Fitting\n",
    "Curve fitting estimates unknown parameters of a model from observed data. It is widely used in growth forecasting and equipment degradation analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1: App user growth forecasting\n",
      "Fitted K=19945.40, r=0.2442, P0=1158.73\n",
      "RMSE: 194.18\n",
      "\n",
      "Example 2: Battery capacity degradation\n",
      "Fitted parameters: a=24.1415, k=0.002850, c0=76.0548\n",
      "RMSE: 0.2812\n",
      "Estimated cycles at 80% capacity: 635.6\n"
     ]
    }
   ],
   "source": [
    "rng = np.random.default_rng(21)\n",
    "months = np.arange(0, 25)\n",
    "\n",
    "def logistic_growth(t, K, r, P0):\n",
    "    return K / (1.0 + ((K - P0) / P0) * np.exp(-r * t))\n",
    "\n",
    "users_true = logistic_growth(months, K=20000.0, r=0.24, P0=1200.0)\n",
    "users_obs = users_true + rng.normal(0, 180, size=months.size)\n",
    "\n",
    "fit_logistic, _ = optimize.curve_fit(\n",
    "    logistic_growth,\n",
    "    months,\n",
    "    users_obs,\n",
    "    p0=[15000.0, 0.2, 900.0],\n",
    "    bounds=([5000.0, 0.01, 100.0], [50000.0, 1.0, 5000.0])\n",
    ")\n",
    "users_pred = logistic_growth(months, *fit_logistic)\n",
    "\n",
    "print(\"Example 1: App user growth forecasting\")\n",
    "print(f\"Fitted K={fit_logistic[0]:.2f}, r={fit_logistic[1]:.4f}, P0={fit_logistic[2]:.2f}\")\n",
    "print(f\"RMSE: {rmse(users_obs, users_pred):.2f}\")\n",
    "\n",
    "def capacity_curve(cycles, a, k, c0):\n",
    "    return a * np.exp(-k * cycles) + c0\n",
    "\n",
    "cycle_count = np.arange(0, 801, 20)\n",
    "capacity_true = capacity_curve(cycle_count, a=24.0, k=0.0028, c0=76.0)\n",
    "capacity_obs = capacity_true + rng.normal(0, 0.35, size=cycle_count.size)\n",
    "\n",
    "fit_capacity, _ = optimize.curve_fit(\n",
    "    capacity_curve,\n",
    "    cycle_count,\n",
    "    capacity_obs,\n",
    "    p0=[20.0, 0.002, 75.0],\n",
    "    bounds=([5.0, 0.0001, 60.0], [40.0, 0.01, 90.0])\n",
    ")\n",
    "capacity_pred = capacity_curve(cycle_count, *fit_capacity)\n",
    "\n",
    "ratio = (80.0 - fit_capacity[2]) / fit_capacity[0]\n",
    "if fit_capacity[0] > 0 and fit_capacity[1] > 0 and 0 < ratio < 1:\n",
    "    cycles_at_80 = -np.log(ratio) / fit_capacity[1]\n",
    "else:\n",
    "    cycles_at_80 = np.nan\n",
    "\n",
    "print()\n",
    "print(\"Example 2: Battery capacity degradation\")\n",
    "print(f\"Fitted parameters: a={fit_capacity[0]:.4f}, k={fit_capacity[1]:.6f}, c0={fit_capacity[2]:.4f}\")\n",
    "print(f\"RMSE: {rmse(capacity_obs, capacity_pred):.4f}\")\n",
    "print(f\"Estimated cycles at 80% capacity: {cycles_at_80:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Activation Function Fitting\n",
    "Activation fitting is useful when you want a smooth function that matches observed response data in neural or probabilistic systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1: Noisy neuron response fitting\n",
      "Sigmoid R^2: 0.9921\n",
      "Tanh R^2: 0.9923\n",
      "Softplus R^2: 0.9443\n",
      "\n",
      "Example 2: Probability calibration from model scores\n",
      "Fitted L=0.9919, threshold=0.3813, slope=2.2798\n",
      "Score near 50% probability: 0.3813\n"
     ]
    }
   ],
   "source": [
    "rng = np.random.default_rng(99)\n",
    "x = np.linspace(-4, 4, 160)\n",
    "\n",
    "def sigmoid(x, L, x0, k):\n",
    "    z = np.clip(k * (x - x0), -60, 60)\n",
    "    return L / (1.0 + np.exp(-z))\n",
    "\n",
    "def tanh_activation(x, a, b, c, d):\n",
    "    return a * np.tanh(b * (x - c)) + d\n",
    "\n",
    "def softplus(x, a, b, c, d):\n",
    "    z = np.clip(b * (x - c), -60, 60)\n",
    "    return a * np.log1p(np.exp(z)) + d\n",
    "\n",
    "y_true = sigmoid(x, 1.0, 0.2, 1.6)\n",
    "y_obs = np.clip(y_true + rng.normal(0, 0.04, size=x.size), 0.0, 1.2)\n",
    "\n",
    "fit_sigmoid, _ = optimize.curve_fit(sigmoid, x, y_obs, p0=[1.0, 0.0, 1.0], bounds=([0.5, -2.0, 0.2], [1.5, 2.0, 4.0]))\n",
    "fit_tanh, _ = optimize.curve_fit(tanh_activation, x, y_obs, p0=[0.5, 1.0, 0.0, 0.5], bounds=([0.1, 0.1, -2.0, 0.0], [1.5, 4.0, 2.0, 1.0]))\n",
    "fit_softplus, _ = optimize.curve_fit(softplus, x, y_obs, p0=[0.3, 1.0, 0.0, 0.0], bounds=([0.05, 0.1, -2.0, -0.2], [2.0, 4.0, 2.0, 0.8]))\n",
    "\n",
    "pred_sigmoid = sigmoid(x, *fit_sigmoid)\n",
    "pred_tanh = tanh_activation(x, *fit_tanh)\n",
    "pred_softplus = softplus(x, *fit_softplus)\n",
    "\n",
    "print(\"Example 1: Noisy neuron response fitting\")\n",
    "print(f\"Sigmoid R^2: {r2_score(y_obs, pred_sigmoid):.4f}\")\n",
    "print(f\"Tanh R^2: {r2_score(y_obs, pred_tanh):.4f}\")\n",
    "print(f\"Softplus R^2: {r2_score(y_obs, pred_softplus):.4f}\")\n",
    "\n",
    "scores = np.linspace(-3, 3, 140)\n",
    "prob_true = sigmoid(scores, 1.0, 0.4, 2.3)\n",
    "prob_obs = np.clip(prob_true + rng.normal(0, 0.03, size=scores.size), 0.0, 1.0)\n",
    "fit_calibration, _ = optimize.curve_fit(\n",
    "    sigmoid,\n",
    "    scores,\n",
    "    prob_obs,\n",
    "    p0=[1.0, 0.0, 1.0],\n",
    "    bounds=([0.8, -2.0, 0.1], [1.2, 2.0, 5.0])\n",
    ")\n",
    "\n",
    "print()\n",
    "print(\"Example 2: Probability calibration from model scores\")\n",
    "print(f\"Fitted L={fit_calibration[0]:.4f}, threshold={fit_calibration[1]:.4f}, slope={fit_calibration[2]:.4f}\")\n",
    "print(f\"Score near 50% probability: {fit_calibration[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Root Finding\n",
    "Root finding solves equations of the form f(x)=0, which is useful for equilibrium analysis and reverse engineering unknown rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1: Market equilibrium\n",
      "Equilibrium price (brentq): 14.4423\n",
      "Equilibrium price (newton): 14.4423\n",
      "Equilibrium quantity: 533.5273\n",
      "\n",
      "Example 2: Implied loan interest rate\n",
      "Monthly rate from brentq: 0.004200\n",
      "Monthly rate from newton: 0.004200\n",
      "Annual nominal rate: 5.0400%\n"
     ]
    }
   ],
   "source": [
    "def demand(price):\n",
    "    return 1100.0 - 45.0 * price + 0.4 * price ** 2\n",
    "\n",
    "def supply(price):\n",
    "    return 150.0 + 28.0 * price - 0.1 * price ** 2\n",
    "\n",
    "def excess_demand(price):\n",
    "    return demand(price) - supply(price)\n",
    "\n",
    "price_brent = optimize.brentq(excess_demand, 0.0, 40.0)\n",
    "price_newton = optimize.newton(excess_demand, x0=10.0)\n",
    "quantity_eq = demand(price_brent)\n",
    "\n",
    "print(\"Example 1: Market equilibrium\")\n",
    "print(f\"Equilibrium price (brentq): {price_brent:.4f}\")\n",
    "print(f\"Equilibrium price (newton): {price_newton:.4f}\")\n",
    "print(f\"Equilibrium quantity: {quantity_eq:.4f}\")\n",
    "\n",
    "principal = 300000.0\n",
    "months = 360\n",
    "true_monthly_rate = 0.0042\n",
    "payment = principal * true_monthly_rate * (1 + true_monthly_rate) ** months / ((1 + true_monthly_rate) ** months - 1)\n",
    "\n",
    "def payment_error(monthly_rate):\n",
    "    growth = (1 + monthly_rate) ** months\n",
    "    return principal * monthly_rate * growth / (growth - 1) - payment\n",
    "\n",
    "rate_brent = optimize.brentq(payment_error, 1e-6, 0.03)\n",
    "rate_newton = optimize.newton(payment_error, x0=0.005)\n",
    "\n",
    "print()\n",
    "print(\"Example 2: Implied loan interest rate\")\n",
    "print(f\"Monthly rate from brentq: {rate_brent:.6f}\")\n",
    "print(f\"Monthly rate from newton: {rate_newton:.6f}\")\n",
    "print(f\"Annual nominal rate: {12 * rate_brent:.4%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. System of Nonlinear Equations\n",
    "Systems of nonlinear equations solve multiple coupled unknowns together. This is common when variables depend on each other through nonlinear relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1: Toll and traffic equilibrium\n",
      "Success: True\n",
      "Toll: 148.9122\n",
      "Traffic flow: 428.5326\n",
      "Residual norm: 8.99e-12\n",
      "\n",
      "Example 2: 2D sensor localization\n",
      "Estimated location: [2. 3.]\n",
      "True location: [2. 3.]\n",
      "Residual norm: 0.00e+00\n"
     ]
    }
   ],
   "source": [
    "def traffic_system(vars_):\n",
    "    toll, flow = vars_\n",
    "    return [\n",
    "        flow - 4000.0 * np.exp(-0.015 * toll),\n",
    "        toll - (2.0 + 0.0008 * flow ** 2)\n",
    "    ]\n",
    "\n",
    "traffic_solution = optimize.root(traffic_system, x0=[20.0, 1500.0], method=\"hybr\")\n",
    "\n",
    "toll_star, flow_star = traffic_solution.x\n",
    "\n",
    "print(\"Example 1: Toll and traffic equilibrium\")\n",
    "print(f\"Success: {traffic_solution.success}\")\n",
    "print(f\"Toll: {toll_star:.4f}\")\n",
    "print(f\"Traffic flow: {flow_star:.4f}\")\n",
    "print(f\"Residual norm: {np.linalg.norm(traffic_solution.fun):.2e}\")\n",
    "\n",
    "true_location = np.array([2.0, 3.0])\n",
    "anchor_a = np.array([0.0, 0.0])\n",
    "anchor_b = np.array([4.0, 1.0])\n",
    "distance_a = np.linalg.norm(true_location - anchor_a)\n",
    "distance_b = np.linalg.norm(true_location - anchor_b)\n",
    "\n",
    "def location_system(vars_):\n",
    "    x_pos, y_pos = vars_\n",
    "    return [\n",
    "        (x_pos - anchor_a[0]) ** 2 + (y_pos - anchor_a[1]) ** 2 - distance_a ** 2,\n",
    "        (x_pos - anchor_b[0]) ** 2 + (y_pos - anchor_b[1]) ** 2 - distance_b ** 2\n",
    "    ]\n",
    "\n",
    "location_solution = optimize.root(location_system, x0=[1.0, 2.0], method=\"hybr\")\n",
    "\n",
    "print()\n",
    "print(\"Example 2: 2D sensor localization\")\n",
    "print(f\"Estimated location: {location_solution.x}\")\n",
    "print(f\"True location: {true_location}\")\n",
    "print(f\"Residual norm: {np.linalg.norm(location_solution.fun):.2e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
